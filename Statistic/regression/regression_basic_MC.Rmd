---
title: "Matemática da regressão linear - Básico"
author: "Luiz Paulo Tavares"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Dependências computacionais 

```{r}
rm(list = ls())
graphics.off()
set.seed(123)

pacman::p_load(tidyverse,
               stats)
```

## **Da equação da reta à regressão linear** 

É intuitivo iniciar a exposição de regressão linear no $\mathbb{R}^2$ tomando como ponto de partida a equação da reta: 

$$
\mathbb{R}^2 = \mathbb{R} \times \mathbb{R} = \{(x,y) \mid x,y \in \mathbb{R}\}
$$

$$
y = a + bx
$$

Estatisticamente, uma dada regressão linear simples busca estimar a inclinação da reta, ou seja, estimar uma reta que minimiza os pontos entre $x$ e $y$ de intersecção dado uma determinada dispersão. Assim, pode-se especificar um modelo de regressão tomando uma variável dependente $Y\in \mathbb{R}^2$ dado um vetor $x = (x_{1},...,x_{d}) \in \mathbb{R}^d$: 

$$
\mathbb{R}^d = \mathbb{R} \times \mathbb{R} \times \dots \times \mathbb{R} = \{(x_1, x_2, \dots, x_d) \mid x_1, x_2, \dots, x_d \in \mathbb{R}\}
$$
Em uma notação convencional (isto é, não matricial) e simplificada, temos: 

$$
Y_{i} = \beta_{1}+\beta_{2}X_{i}+\mu_{i}
$$

Observe a adição de $\mu$ representando os resíduos da regressão. Os quais são em essência a diferenças entre os valores observados e estimados. Por sua vez, ${\beta_{1}}$ e ${\beta_{2}}$ representam o intercepto e coeficiente angular da equação da reta, respectivamente. Assim, estima-se dado uma amostra $n$ com variabilidade: 

$$
Y_{i} = \hat{\beta_{1}}+{\hat{\beta_{2}}X_{i}}+{\hat{\mu}_{i}}
$$
Os resíduos: 

$$
{\hat{\mu}_{i}} = {Y} - {\hat{Y_{i}}}
$$
Portanto: 

$$
\hat{\mu_{i}} = Y_{i} - {\hat{\beta}_{1}}-{\hat{\beta}_{2}X_{i}}
$$

Do exposto, desenvolve-se por consequência a busca por encontrar ou, melhor, minimizar os resíduos encontrando valores próximos de $Y$
 observado. Critério bem difundido na literatura, pelo menos desde Friedrich Gauss, é o método dos mínimos quadrados. O qual, como suguere, busca minimizar os resíduos (GUJARATI & PORTER, 2011). Tomando como ponto de partida:

$$
{\sum_{i =1}^n \mu^2_{i}} = \sum(Y_i - {\hat{Y_{i}}})^2
$$
$$
= \sum_{i=1}^n(Y_i - {\hat{\beta_{1}}}-{\hat{\beta}_{2}}X_{i})^2
$$


Posteriormente, a minimização via MQO será retomada. Por enquanto, tenha como dado que minimizar os resíduos como exposto e chegar num métrica de ajuste do modelo estimado: coeficiente de determinado R²: 

$$
SQE = \sum_{i = 1}^n{(\hat{Y_{i}}-\overline{Y})^2}
$$


$$
SQT = \sum_{i = 1}^n{(Y_{i}-\overline{Y})^2}
$$


$$
R² = \frac{SQE}{SQT}
$$


## **Dispersão**

Pode-se estimar considerando apenas uma variável indepedente com $n$ = 1000 com $X$ ~ $N(0,\sigma^2)$: 

```{r}
# Modelo de regresssão linear simples 

y = rnorm(n = 1000)
x = rnorm(n = 1000, mean = 0, sd = 1) 

bd_model = data.frame(depedente = y, explicativa = x)

model_lm = stats::lm(formula = depedente ~ explicativa, data = bd_model) %>% print()
```

Assim, estima-se $\hat{\beta_{0}}$ e $\hat{\beta_{1}}$, ou seja, o intercepto e o coeficiente angular da reta de regressão linear, respectivamente. O interceto retornou aproximadamente 0.12 e, por outro lado, o coefiente angular 0.85. 
É interessante nota que matematicamente a inclinação da reta pode ser encontrada como segue: 

$$
\hat{\beta_{2}} = \frac{rs_{y}}{s_{x}}
$$
Ou seja, dado a correlação de Pearson $r$ entre $x$ e $y$ multiplicado pelo desvio padrão $s_{y}$ dividido pelo desvio padrão de $s_{x}$. Assim, temos o resultado de aproximadamente 0.85 para o coefiente angular encontrado anteriormente. Por questão de dúvida, em seguida define-se a correlação e desvio padrão: 

$$
Cov(X,Y) = \frac{\sum_{i=1}^n{(x_{i}-\overline{x})(y_{i}-\overline{y})}}{n-1}
$$
$$
r = \frac{cov(X,Y)}{s_{x}s_{y}}
$$
e desvio padeão: 

$$
s = \sqrt{\frac{1}{N} \sum_{i=1}^n{(x_{i}-\overline{x})^2}}
$$


```{r}
b_2 = (stats::cor(x,y) * stats::sd(y)) / (stats::sd(x))
print(b_2)
```

para b_zero
E o intercepto 
$$
\beta_{1} = \overline{y} - \beta_{2}\overline{x}
$$
```{r}
#b_1 = base::mean(y) - (model_lm[["coefficients"]][["x"]] *base::mean(x)) 
#print(b_1)
```

Ou seja, uma alta dispersão, principalmente, seguindo um padrão não linear pode em muito prejudicar o ajuste de uma reta linear entre $x$ e $y$ de minimização. Indo além, pode-se tomar a dispersão como $\mu$: 


$$
y(x):= E[Y|X = x]
$$
